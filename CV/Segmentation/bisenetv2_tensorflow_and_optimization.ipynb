{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bisenetv2-tensorflow-and-optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fKTzuTs2V-pl",
        "1hL3hEQMWEJz"
      ],
      "mount_file_id": "1ulzQ8TLqwpSarcUB-UNiiUDl0Tz5veWl",
      "authorship_tag": "ABX9TyNmMU/57AQxKDnWyhf/X/K/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProtossDragoon/CoMoLab/blob/master/CV/Segmentation/bisenetv2_tensorflow_and_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRZ9tGbkfGdu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d3583d-9a8a-4de2-f686-e6164931361e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtlT_1YvXLHK",
        "outputId": "563142c5-12f6-4b24-a25e-2a712aaec764"
      },
      "source": [
        "!nvcc -V"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXnYP97JXFXe",
        "outputId": "1930b51d-9774-47bc-a439-88287cc3b2e2"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Dec  8 07:12:20 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFBo0uoNgkHB"
      },
      "source": [
        "Install TensorRT Runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAhK7ulcgK_I",
        "outputId": "8e905108-8716-41ba-be68-d1e3687d6797"
      },
      "source": [
        "%%bash\n",
        "wget https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
        "\n",
        "dpkg -i nvidia-machine-learning-repo-*.deb\n",
        "apt-get update\n",
        "\n",
        "sudo apt-get install libnvinfer5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package nvidia-machine-learning-repo-ubuntu1804.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb ...\n",
            "Unpacking nvidia-machine-learning-repo-ubuntu1804 (1.0.0-1) ...\n",
            "Setting up nvidia-machine-learning-repo-ubuntu1804 (1.0.0-1) ...\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,370 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,696 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,134 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,243 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [868 kB]\n",
            "Get:19 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [46.5 kB]\n",
            "Fetched 8,650 kB in 3s (3,252 kB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following NEW packages will be installed:\n",
            "  libnvinfer5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 46.7 MB of archives.\n",
            "After this operation, 163 MB of additional disk space will be used.\n",
            "Get:1 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libnvinfer5 5.1.5-1+cuda10.1 [46.7 MB]\n",
            "Fetched 46.7 MB in 1s (60.3 MB/s)\n",
            "Selecting previously unselected package libnvinfer5.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 144868 files and directories currently installed.)\r\n",
            "Preparing to unpack .../libnvinfer5_5.1.5-1+cuda10.1_amd64.deb ...\r\n",
            "Unpacking libnvinfer5 (5.1.5-1+cuda10.1) ...\r\n",
            "Setting up libnvinfer5 (5.1.5-1+cuda10.1) ...\r\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\r\n",
            "\r\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "--2020-12-08 04:07:54--  https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2926 (2.9K) [application/x-deb]\n",
            "Saving to: ‘nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb’\n",
            "\n",
            "     0K ..                                                    100%  146M=0s\n",
            "\n",
            "2020-12-08 04:07:54 (146 MB/s) - ‘nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb’ saved [2926/2926]\n",
            "\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/nvidia-machine-learning.list:1 and /etc/apt/sources.list.d/nvidia-ml.list:1\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/nvidia-machine-learning.list:1 and /etc/apt/sources.list.d/nvidia-ml.list:1\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/nvidia-machine-learning.list:1 and /etc/apt/sources.list.d/nvidia-ml.list:1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBJF3QqmesAj",
        "outputId": "3ea93a91-03f1-49dc-a565-aea4ab434bfd"
      },
      "source": [
        "# check TensorRT version\n",
        "print(\"TensorRT version: \")\n",
        "!dpkg -l | grep nvinfer"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorRT version: \n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkG0icMnHRI5"
      },
      "source": [
        "%cd /content/gdrive/\"My Drive\"/WhitePaper\n",
        "!git clone https://github.com/ProtossDragoon/bisenetv2-tensorflow.git\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tP42W9nfvdh",
        "outputId": "c91c7323-c602-4e8b-edac-6b1b8e789351"
      },
      "source": [
        "try:\n",
        "  # This %tensorflow_version magic only works in Colab.\n",
        "  %tensorflow_version 1.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "# For your non-Colab code, be sure you have tensorflow==1.15\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('1')\n",
        "print(tf.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npY0PIUwf0z1",
        "outputId": "2306e713-ceae-4cf1-f9d4-b33293efe77c"
      },
      "source": [
        "%cd /content/gdrive/\"My Drive\"/WhitePaper/bisenetv2-tensorflow\n",
        "%ls -al\n",
        "\n",
        "# 프로그램 실행 시 반드시 bisenetv2-tensorflow 위치로 접근할 것.\n",
        "# --weights_path 은 정확한 이름이 아니라 ckpt 까지만 지정해 주면 됨. xxxx of xxxx 파일도 반드시 필요함.\n",
        "# --src_image_path 을 원래 이미지 하나만 지정해야 동작했는데, 경로로 지정하면 거기에 있는 모든 이미지를 전부 segmentation 하도록 코드를 편집함.\n",
        "# test_bisenetv2_cityscapes.py 만 수정한 상태임.\n",
        "!python3 ./tools/cityscapes/test_bisenetv2_cityscapes.py --weights_path ./model/cityscapes/bisenetv2/1/cityscapes.ckpt --src_image_path ./data/test_image/cityscapes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow\n",
            "total 62\n",
            "drwx------ 2 root root 4096 Dec  1 04:11 \u001b[0m\u001b[01;34mbisenet_model\u001b[0m/\n",
            "drwx------ 2 root root 4096 Dec  1 04:11 \u001b[01;34mconfig\u001b[0m/\n",
            "-rw------- 1 root root   26 Dec  1 04:11 _config.yml\n",
            "drwx------ 2 root root 4096 Dec  1 04:11 \u001b[01;34mdata\u001b[0m/\n",
            "drwx------ 2 root root 4096 Dec  1 14:30 \u001b[01;34mdata_output\u001b[0m/\n",
            "drwx------ 2 root root 4096 Dec  1 04:11 \u001b[01;34mdata_provider\u001b[0m/\n",
            "drwx------ 2 root root 4096 Dec  1 05:56 \u001b[01;34m.git\u001b[0m/\n",
            "-rw------- 1 root root 1530 Dec  1 04:11 .gitignore\n",
            "drwx------ 2 root root 4096 Dec  1 14:30 \u001b[01;34m.ipynb_checkpoints\u001b[0m/\n",
            "-rw------- 1 root root 1072 Dec  1 04:11 LICENSE\n",
            "drwx------ 2 root root 4096 Dec  1 04:11 \u001b[01;34mlocal_utils\u001b[0m/\n",
            "drwx------ 2 root root 4096 Dec  1 04:13 \u001b[01;34mmodel\u001b[0m/\n",
            "-rw------- 1 root root 9492 Dec  1 04:11 README.md\n",
            "-rw------- 1 root root  127 Dec  1 04:11 requirements.txt\n",
            "drwx------ 2 root root 4096 Dec  1 04:11 \u001b[01;34mscripts\u001b[0m/\n",
            "drwx------ 2 root root 4096 Dec  1 04:11 \u001b[01;34mtools\u001b[0m/\n",
            "drwx------ 2 root root 4096 Dec  1 04:11 \u001b[01;34mtrainner\u001b[0m/\n",
            "-------version-------\n",
            "tensorflow version 1.15.2\n",
            "-------version-------\n",
            "-------path parsing-------\n",
            "/content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow\n",
            "model/cityscapes/bisenetv2/cityscapes.ckpt\n",
            "/content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/model/cityscapes/bisenetv2/cityscapes.ckpt\n",
            "data/test_image/cityscapes\n",
            "/content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data/test_image/cityscapes\n",
            "this is a path\n",
            "-------path parsing-------\n",
            "WARNING:tensorflow:From ./tools/cityscapes/test_bisenetv2_cityscapes.py:129: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/bisenet_model/bisenet_v2.py:1138: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/bisenet_model/cnn_basenet.py:71: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/bisenet_model/cnn_basenet.py:401: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/bisenet_model/cnn_basenet.py:222: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/bisenet_model/cnn_basenet.py:246: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/bisenet_model/bisenet_v2.py:583: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.\n",
            "\n",
            "WARNING:tensorflow:From ./tools/cityscapes/test_bisenetv2_cityscapes.py:142: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From ./tools/cityscapes/test_bisenetv2_cityscapes.py:146: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2020-12-08 04:08:28.048266: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2020-12-08 04:08:28.048565: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2666840 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-12-08 04:08:28.048608: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-12-08 04:08:28.052396: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-12-08 04:08:28.245080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-08 04:08:28.245830: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2666d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-12-08 04:08:28.245880: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-12-08 04:08:28.247049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-08 04:08:28.247595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-12-08 04:08:28.247905: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-12-08 04:08:28.452511: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-12-08 04:08:28.566674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-12-08 04:08:28.593195: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-12-08 04:08:28.833506: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-12-08 04:08:28.881496: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-12-08 04:08:29.344583: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-12-08 04:08:29.344780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-08 04:08:29.345519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-08 04:08:29.346059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-12-08 04:08:29.349093: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-12-08 04:08:29.350471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-12-08 04:08:29.350499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2020-12-08 04:08:29.350511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2020-12-08 04:08:29.351444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-08 04:08:29.352035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-08 04:08:29.352550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13571 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From ./tools/cityscapes/test_bisenetv2_cityscapes.py:155: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "['test_01.png', 'test_02.png', 'test_03.png', '.ipynb_checkpoints', '11.00.17.png', '11.13.16.png', '11.13.10.png', '11.13.05.png', '11.12.59.png', '11.00.30.png', '11.12.52.png', '11.12.46.png', '11.12.41.png', '11.12.36.png', '11.12.32.png', '11.12.28.png', '11.12.22.png', '11.12.15.png', '11.12.10.png', '11.00.44.png', '11.00.51.png', '11.01.08.png', '11.11.46.png', '11.11.56.png', '11.12.05.png', '10.59.46.png']\n",
            "25 image(s) detected\n",
            "image test_01.png reading\n",
            "image test_01.png : shape (1024, 2048, 3)\n",
            "2020-12-08 04:08:41.144121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-12-08 04:08:46.200854: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/test_01.png_output.jpg\n",
            "\n",
            "image test_02.png reading\n",
            "image test_02.png : shape (1024, 2048, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  5  6  7  8  9 10 11 13]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/test_02.png_output.jpg\n",
            "\n",
            "image test_03.png reading\n",
            "image test_03.png : shape (1024, 2048, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  4  5  7  8 10 11 13 14]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/test_03.png_output.jpg\n",
            "\n",
            "image 11.00.17.png reading\n",
            "image 11.00.17.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  7  8  9 10 11 13 14 15 17 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.00.17.png_output.jpg\n",
            "\n",
            "image 11.13.16.png reading\n",
            "image 11.13.16.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.13.16.png_output.jpg\n",
            "\n",
            "image 11.13.10.png reading\n",
            "image 11.13.10.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 15 16 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.13.10.png_output.jpg\n",
            "\n",
            "image 11.13.05.png reading\n",
            "image 11.13.05.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.13.05.png_output.jpg\n",
            "\n",
            "image 11.12.59.png reading\n",
            "image 11.12.59.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 15 17 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.12.59.png_output.jpg\n",
            "\n",
            "image 11.00.30.png reading\n",
            "image 11.00.30.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  7  8  9 10 11 13 14 15 16 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.00.30.png_output.jpg\n",
            "\n",
            "image 11.12.52.png reading\n",
            "image 11.12.52.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.12.52.png_output.jpg\n",
            "\n",
            "image 11.12.46.png reading\n",
            "image 11.12.46.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.12.46.png_output.jpg\n",
            "\n",
            "image 11.12.41.png reading\n",
            "image 11.12.41.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  6  7  8  9 11 12 13 15 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.12.41.png_output.jpg\n",
            "\n",
            "image 11.12.36.png reading\n",
            "image 11.12.36.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  7  8 10 11 12 13 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.12.36.png_output.jpg\n",
            "\n",
            "image 11.12.32.png reading\n",
            "image 11.12.32.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.12.32.png_output.jpg\n",
            "\n",
            "image 11.12.28.png reading\n",
            "image 11.12.28.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  7  8  9 10 11 13 14 17 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.12.28.png_output.jpg\n",
            "\n",
            "image 11.12.22.png reading\n",
            "image 11.12.22.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  7  8  9 10 11 13 14 15 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.12.22.png_output.jpg\n",
            "\n",
            "image 11.12.15.png reading\n",
            "image 11.12.15.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 17 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.12.15.png_output.jpg\n",
            "\n",
            "image 11.12.10.png reading\n",
            "image 11.12.10.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.12.10.png_output.jpg\n",
            "\n",
            "image 11.00.44.png reading\n",
            "image 11.00.44.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 17 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.00.44.png_output.jpg\n",
            "\n",
            "image 11.00.51.png reading\n",
            "image 11.00.51.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.00.51.png_output.jpg\n",
            "\n",
            "image 11.01.08.png reading\n",
            "image 11.01.08.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 15 17 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.01.08.png_output.jpg\n",
            "\n",
            "image 11.11.46.png reading\n",
            "image 11.11.46.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.11.46.png_output.jpg\n",
            "\n",
            "image 11.11.56.png reading\n",
            "image 11.11.56.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 17 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.11.56.png_output.jpg\n",
            "\n",
            "image 11.12.05.png reading\n",
            "image 11.12.05.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 17 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/11.12.05.png_output.jpg\n",
            "\n",
            "image 10.59.46.png reading\n",
            "image 10.59.46.png : shape (1920, 2646, 3)\n",
            "Prediction mask unique label ids: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 17 18]\n",
            "save as : /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/data_output/test_image/cityscapes/10.59.46.png_output.jpg\n",
            "\n",
            "Mean cost time (inference ~ reshape ~ mapping): 0.79581s\n",
            "Mean fps (inference ~ reshape ~ mapping): 1.25659fps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUY27El3FzHt"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Sp1hHkXV8SJ"
      },
      "source": [
        "## Convert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_VfTJW3CABP"
      },
      "source": [
        "forzen model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aaSQyYLGd0Q"
      },
      "source": [
        "원래 더 아래에 있는 script 를 사용해야 하겠지만, 요놈의 오픈소스는 pb 파일이 정상적으로 돌아가지를 않는다. 그래서 그냥 오픈소스 자체에서 제공하는 python 스크립트를 활용한다. 이유는, 오픈소스 제작자가 google drive 에 올려둔 ckpt 파일 세 가지의 정보가 미묘하게 다르기 때문일 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0lAOzAnB_wt",
        "outputId": "0ab7e3c8-2fdf-4036-91ae-590708b0c4de"
      },
      "source": [
        "%cd /content/gdrive/\"My Drive\"/WhitePaper/bisenetv2-tensorflow\n",
        "!mkdir checkpoint\n",
        "!python3 ./tools/cityscapes/freeze_cityscapes_bisenetv2_model.py --weights_path ./model/cityscapes/bisenetv2/1/cityscapes.ckpt --frozen_pb_file_path ./checkpoint/bisenetv2_cityscapes_frozen.pb --optimized_pb_file_path ./checkpoint/bisenetv2_cityscapes_optimized.pb"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow\n",
            "mkdir: cannot create directory ‘checkpoint’: File exists\n",
            "WARNING:tensorflow:From ./tools/cityscapes/freeze_cityscapes_bisenetv2_model.py:52: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/bisenet_model/bisenet_v2.py:1138: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/bisenet_model/cnn_basenet.py:71: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/bisenet_model/cnn_basenet.py:401: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/bisenet_model/cnn_basenet.py:222: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/bisenet_model/cnn_basenet.py:246: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow/bisenet_model/bisenet_v2.py:583: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.\n",
            "\n",
            "WARNING:tensorflow:From ./tools/cityscapes/freeze_cityscapes_bisenetv2_model.py:62: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From ./tools/cityscapes/freeze_cityscapes_bisenetv2_model.py:72: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From ./tools/cityscapes/freeze_cityscapes_bisenetv2_model.py:74: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2020-12-08 07:35:02.687665: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2020-12-08 07:35:02.687911: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1792a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-12-08 07:35:02.687943: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-12-08 07:35:02.689857: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-12-08 07:35:02.786375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-08 07:35:02.787098: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1792bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-12-08 07:35:02.787134: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-12-08 07:35:02.787283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-08 07:35:02.787890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-12-08 07:35:02.788180: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-12-08 07:35:02.789700: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-12-08 07:35:02.791377: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-12-08 07:35:02.791694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-12-08 07:35:02.793284: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-12-08 07:35:02.794026: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-12-08 07:35:02.796977: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-12-08 07:35:02.797076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-08 07:35:02.797664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-08 07:35:02.798188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-12-08 07:35:02.798240: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-12-08 07:35:02.799299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-12-08 07:35:02.799327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2020-12-08 07:35:02.799338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2020-12-08 07:35:02.799484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-08 07:35:02.800057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-12-08 07:35:02.800590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13571 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From ./tools/cityscapes/freeze_cityscapes_bisenetv2_model.py:80: The name tf.train.export_meta_graph is deprecated. Please use tf.compat.v1.train.export_meta_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From ./tools/cityscapes/freeze_cityscapes_bisenetv2_model.py:88: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.remove_training_nodes`\n",
            "WARNING:tensorflow:From ./tools/cityscapes/freeze_cityscapes_bisenetv2_model.py:102: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "WARNING:tensorflow:From ./tools/cityscapes/freeze_cityscapes_bisenetv2_model.py:103: The name tf.train.write_graph is deprecated. Please use tf.io.write_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtedMHPaCBBv"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYc50Q4RCBbl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kavptKGdjcZ2"
      },
      "source": [
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oZLR24Rgjcpf",
        "outputId": "83c8be60-dde5-4cb3-9753-9899b8268432"
      },
      "source": [
        "%cd /content/gdrive/\"My Drive\"/WhitePaper/bisenetv2-tensorflow\n",
        "\n",
        "tf.reset_default_graph()\n",
        "graph_1 = tf.Graph()\n",
        "with graph_1.as_default():\n",
        "\tif graph_1 is tf.get_default_graph():\n",
        "\t\tprint('True')\n",
        "\t\n",
        "\twith tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess: # 그래프 연산에 필요한 컴퓨팅 자원 연결\n",
        "\t\tsaver = tf.train.import_meta_graph('model/cityscapes/bisenetv2/0/cityscapes_temp-0.meta')\n",
        "\t\t# 이 op는 default graph에 노드로 들어갈 것입니다.\n",
        "\t\tsaver.restore(sess, 'model/cityscapes/bisenetv2/1/cityscapes.ckpt.data-00000-of-00001')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/WhitePaper/bisenetv2-tensorflow\n",
            "True\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from model/cityscapes/bisenetv2/1/cityscapes.ckpt.data-00000-of-00001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "DataLossError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDataLossError\u001b[0m: 2 root error(s) found.\n  (0) Data loss: Unable to open table file model/cityscapes/bisenetv2/1/cityscapes.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[{{node save/RestoreV2}}]]\n\t [[save/RestoreV2/_301]]\n  (1) Data loss: Unable to open table file model/cityscapes/bisenetv2/1/cityscapes.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[{{node save/RestoreV2}}]]\n0 successful operations.\n0 derived errors ignored.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-77f166900a83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model/cityscapes/bisenetv2/0/cityscapes_temp-0.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0;31m# 이 op는 default graph에 노드로 들어갈 것입니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model/cityscapes/bisenetv2/1/cityscapes.ckpt.data-00000-of-00001'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1290\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1291\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m       \u001b[0;31m# There are three common conditions that might cause this error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDataLossError\u001b[0m: 2 root error(s) found.\n  (0) Data loss: Unable to open table file model/cityscapes/bisenetv2/1/cityscapes.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[node save/RestoreV2 (defined at /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py:1748) ]]\n\t [[save/RestoreV2/_301]]\n  (1) Data loss: Unable to open table file model/cityscapes/bisenetv2/1/cityscapes.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[node save/RestoreV2 (defined at /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py:1748) ]]\n0 successful operations.\n0 derived errors ignored.\n\nOriginal stack trace for 'save/RestoreV2':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 462, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 492, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 444, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-77f166900a83>\", line 10, in <module>\n    saver = tf.train.import_meta_graph('model/cityscapes/bisenetv2/0/cityscapes_temp-0.meta')\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py\", line 1453, in import_meta_graph\n    **kwargs)[0]\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py\", line 1477, in _import_meta_graph_with_return_elements\n    **kwargs))\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/meta_graph.py\", line 809, in import_scoped_meta_graph_with_return_elements\n    return_elements=return_elements)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/importer.py\", line 405, in import_graph_def\n    producer_op_list=producer_op_list)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/importer.py\", line 517, in _import_graph_def_internal\n    _ProcessNewOps(graph)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/importer.py\", line 243, in _ProcessNewOps\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 3561, in _add_new_tf_operations\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 3561, in <listcomp>\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 3451, in _create_op_from_tf_operation\n    ret = Operation(c_op, self)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY62qTfkjogp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOAjkJXfjc5R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvqfHFDZjdE7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zFSHv0LY7eM"
      },
      "source": [
        "### way [tf-trt] : (Recommend) keras saved model (not .h5 model) to tensorflow graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9jqtC2fw6__"
      },
      "source": [
        "kears dptj 전체 모델을 디스크에 저장하는 데 사용할 수 있는 두 형식은 TensorFlow SavedModel 형식과 이전 Keras H5 형식입니다. 권장하는 형식은 SavedModel입니다. 이는 model.save()를 사용할 때의 기본값입니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "다음을 통해 H5 형식으로 전환할 수 있습니다.\n",
        "\n",
        "- format='h5'를 save()로 전달합니다.\n",
        "- .h5 또는 .keras로 끝나는 파일명을 save()로 전달합니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "about savedmodel\n",
        "\n",
        "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md\n",
        "\n",
        "<br>\n",
        "\n",
        "savedmodel tutorial, savedmodel cli example\n",
        "\n",
        "https://www.tensorflow.org/guide/saved_model?hl=ko"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_A9Pr49qgMs"
      },
      "source": [
        "Case : Float32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4ajI-6CY8AP"
      },
      "source": [
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
        "import tensorflow as tf\n",
        "\n",
        "# load keras (savedmodel, not h5) model\n",
        "model = tf.keras.models.load_model('path/1/modelname')\n",
        "\n",
        "print('Converting to TF-TRT FP32...')\n",
        "# define parameter\n",
        "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.FP32,\n",
        "                                                               max_workspace_size_bytes=8000000000)\n",
        "# define converter\n",
        "converter = trt.TrtGraphConverterV2(input_saved_model_dir='path/1/modelname',\n",
        "                                    conversion_params=conversion_params)\n",
        "# convert\n",
        "converter.convert()\n",
        "converter.save(output_saved_model_dir='modelname_saved_model_TFTRT_FP32')\n",
        "print('Done Converting to TF-TRT FP32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4RmGptPq1JB"
      },
      "source": [
        "Case : Float16\n",
        "\n",
        "\n",
        "We next convert the native TF FP32 saved model to TF-TRT FP16 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "8PA6RWz6q0-j",
        "outputId": "3a0bb4b1-d3b9-42e8-e2d2-06fea0788981"
      },
      "source": [
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
        "import tensorflow as tf\n",
        "\n",
        "# load keras (savedmodel, not h5) model\n",
        "model = tf.keras.models.load_model('path/modelname/1/')\n",
        "'''\n",
        "저장 경로의 마지막 경로 요소(여기서는 1/)는 모델의 버전 번호인 텐서플로 서빙(TensorFlow Serving) 컨벤션을 따릅니다.\n",
        "텐서플로 서빙과 같은 도구가 최신 모델을 구분할 수 있게 합니다.\n",
        "'''\n",
        "\n",
        "print('Converting to TF-TRT FP16...')\n",
        "\n",
        "                                                               \n",
        "\n",
        "# tensorflow 2.x\n",
        "'''\n",
        "# define parameter\n",
        "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.FP16,\n",
        "                                                               max_workspace_size_bytes=8000000000)\n",
        "\n",
        "#params\n",
        "# precision_mode: The precision mode to use (FP32, FP16, or INT8)\n",
        "# minimum_segment_size: The minimum number of TensorFlow nodes required for a TensorRT subgraph to be valid.\n",
        "# is_dynamic_op: TensorRT engines are converted and built at model run time instead of during the converter.convert() call. This is required if there are tensors with unknown or dynamic shapes.\n",
        "# use_calibration: Only used if precision_mode='INT8'. If True, a calibration graph will be created, and converter.calibrate() should be called. This is the recommended option. \n",
        "                    If False, all tensors that will not be fused must have quantization nodes. See NVIDIA’s INT8 Quantization for details.\n",
        "# max_batch_size: Used when is_dynamic_op=False. This is the maximum batch size for TensorRT engines. At run time, smaller batch sizes can be used, but a larger batch size will result in an error.\n",
        "# maximum_cached_engines: Used when is_dynamic_op=True. This limits the number of TensorRT engines that are cached, per TRTEngineOp.\n",
        "\n",
        "# define converter\n",
        "converter = trt.TrtGraphConverterV2(input_saved_model_dir='modelname',\n",
        "                                    conversion_params=conversion_params)\n",
        "'''\n",
        "\n",
        "# tensorflow 1.x : case 1\n",
        "'''\n",
        "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.FP16,\n",
        "                                                               max_workspace_size_bytes=8000000000)\n",
        "converter = trt.TrtGraphConverter(input_saved_model_dir='path/modelname/1/',\n",
        "                                  conversion_params=conversion_params)\n",
        "'''\n",
        "\n",
        "# tensorflow 1.x : case 2\n",
        "converter = trt.TrtGraphConverter(input_saved_model_dir='path/modelname/1/',\n",
        "                                  precision_mode='FP16',\n",
        "                                  is_dynamic_op=False\n",
        "                                  max_batch_size=1\n",
        "                                  max_workspace_size_bytes=(11<32),\n",
        "                                  )\n",
        "\n",
        "# convert\n",
        "converter.convert()\n",
        "converter.save(output_saved_model_dir='path/modelname_saved_model_TFTRT_FP16/1/')\n",
        "print('Done Converting to TF-TRT FP16')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-2bfc035bcd13>\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    precision_mode=”FP16”,\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OztCQbf_q3Md"
      },
      "source": [
        "Case : Int8\n",
        "\n",
        "Creating TF-TRT INT8 model requires a small calibration dataset. This data set ideally should represent the test data in production well, and will be used to create a value histogram for each layer in the neural network for effective 8-bit quantization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebAq_pBzq3g4"
      },
      "source": [
        "batch_size = 8\n",
        "W = 224\n",
        "H = 224\n",
        "C = 3\n",
        "\n",
        "batched_input = np.zeros((batch_size, H, W, C), dtype=np.float32)\n",
        "\n",
        "for i in range(batch_size):\n",
        "  img_path = './data/img%d.JPG' %(i)\n",
        "  img = image.load_img(img_path, target_size=(H, W))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  x = preprocess_input(x)\n",
        "  batched_input[i, :] = x\n",
        "\n",
        "batched_input = tf.constant(batched_input)\n",
        "print('batched_input shape: ', batched_input.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juAk_ceZtQj0"
      },
      "source": [
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
        "import tensorflow as tf\n",
        "\n",
        "# load keras (savedmodel, not h5) model\n",
        "model = tf.keras.models.load_model('modelname')\n",
        "\n",
        "print('Converting to TF-TRT INT8...')\n",
        "# define parameter\n",
        "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.INT8,\n",
        "                                                               max_workspace_size_bytes=8000000000)\n",
        "# define converter\n",
        "converter = trt.TrtGraphConverterV2(input_saved_model_dir='modelname.h5',\n",
        "                                    conversion_params=conversion_params)\n",
        "# convert\n",
        "def calibration_input_fn():\n",
        "    yield (batched_input, )\n",
        "converter.convert(calibration_input_fn=calibration_input_fn)\n",
        "converter.save(output_saved_model_dir='modelname_saved_model_TFTRT_INT8')\n",
        "print('Done Converting to TF-TRT INT8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9q7Egv7Zjy4"
      },
      "source": [
        "### way 2-2 [tf-trt] : (Recommend) tf savedmodel to TF-TensorRT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl_vBvNCZkLv"
      },
      "source": [
        "# Float 32\n",
        "saved_model_loaded = tf.saved_model.load('path/modelname_saved_model_TFTRT_FP32/1/', tags=[tag_constants.SERVING])\n",
        "\n",
        "# Float 16\n",
        "saved_model_loaded = tf.saved_model.load('path/modelname_saved_model_TFTRT_FP16/1/', tags=[tag_constants.SERVING])\n",
        "\n",
        "# INT 8\n",
        "saved_model_loaded = tf.saved_model.load('path/modelname_saved_model_TFTRT_INT8/1/', tags=[tag_constants.SERVING])\n",
        "\n",
        "'''\n",
        "SavedModel은 시그니처(signatures)라 불리는 이름있는 함수를 가집니다. \n",
        "케라스 모델은 serving_default 시그니처 키를 사용하여 정방향 패스(forward pass)를 내보냅니다. \n",
        "가져온 시그니처는 항상 딕셔너리를 반환합니다.\n",
        "\n",
        "*ckarh : SavedModel 커맨드 라인 인터페이스는 디스크에 저장된 SavedModel을 검사할 때 유용합니다.\n",
        "'''\n",
        "signature_keys = list(saved_model_loaded.signatures.keys())\n",
        "print(signature_keys)\n",
        "# output : ['serving_default']\n",
        "infer = saved_model_loaded.signatures['serving_default']\n",
        "print(infer.structured_outputs)\n",
        "\n",
        "# tensorflow SavedModel -> TensorRT\n",
        "'''\n",
        "If you have a SavedModel representation of your TensorFlow model, \n",
        "you can create a TensorRT inference graph directly from your SavedModel, for example:\n",
        "'''\n",
        "'''\n",
        "with tf.Session() as sess:\n",
        "    # First load the SavedModel into the session    \n",
        "    tf.saved_model.loader.load(\n",
        "        sess, [tf.saved_model.tag_constants.SERVING], path/modelname_saved_model_TFTRT_FP16/1/)\n",
        "    output = sess.run([output_tensor], feed_dict={input_tensor: input_data})\n",
        "'''\n",
        "\n",
        "# tensorflow FrozenGraph (.pb) -> TensorRT\n",
        "'''\n",
        "If you have a frozen graph of your TensorFlow model,\n",
        "you first need to load the frozen graph file and parse it to create a deserialized GraphDef. \n",
        "Then you can use the GraphDef to create a TensorRT inference graph, for example:\n",
        "'''\n",
        "'''\n",
        "with tf.Session() as sess:\n",
        "    # First deserialize your frozen graph:\n",
        "    with tf.gfile.GFile(“/path/to/your/frozen/graph.pb”, ‘rb’) as f:\n",
        "        frozen_graph = tf.GraphDef()\n",
        "        frozen_graph.ParseFromString(f.read())\n",
        "    # Now you can create a TensorRT inference graph from your frozen graph:\n",
        "    converter = trt.TrtGraphConverter(\n",
        "\t    input_graph_def=frozen_graph,\n",
        "\t    nodes_blacklist=['logits', 'classes']) #output nodes\n",
        "    trt_graph = converter.convert()\n",
        "    # Import the TensorRT graph into a new graph and run:\n",
        "    output_node = tf.import_graph_def(\n",
        "        trt_graph,\n",
        "        return_elements=['logits', 'classes'])\n",
        "    sess.run(output_node)\n",
        "'''\n",
        "\n",
        "# tensorflow Metagraph (.meta) -> TensorRT\n",
        "'''\n",
        "If you don’t have a SavedModel or a frozen graph representation of your TensorFlow model \n",
        "but have separate MetaGraph and checkpoint files, \n",
        "you first need to use these to create a SavedModel or a frozen graph to then feed into TF-TRT. \n",
        "The following example shows how to freeze a graph from checkpoints:\n",
        "'''\n",
        "\n",
        "'''\n",
        "with tf.Session() as sess:\n",
        "    # First create a `Saver` object (for saving and rebuilding a\n",
        "    # model) and import your `MetaGraphDef` protocol buffer into it:\n",
        "    saver = tf.train.import_meta_graph(“/path/to/your/model.ckpt.meta”)\n",
        "    # Then restore your training data from checkpoint files:\n",
        "    saver.restore(sess, “/path/to/your/model.ckpt”)\n",
        "    # Finally, freeze the graph:\n",
        "    frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
        "        sess,\n",
        "        tf.get_default_graph().as_graph_def(),\n",
        "        output_node_names=['logits', 'classes'])\n",
        "'''\n",
        "\n",
        "labeling = infer(x)\n",
        "preds = labeling['probs'].numpy\n",
        "print(preds.shape)\n",
        "print(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DthpIaD7oJfu"
      },
      "source": [
        "Creating TF-TRT INT8 model requires a small calibration dataset. This data set ideally should represent the test data in production well, and will be used to create a value histogram for each layer in the neural network for effective 8-bit quantization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY5bkTEooKFU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKTzuTs2V-pl"
      },
      "source": [
        "### way 1-1 [tensorRT] : (deprecated) keras h5 model to frozen .pb model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBSKMLU4Fxv1"
      },
      "source": [
        "le import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.python.framework import graph_io\n",
        "\n",
        "\n",
        "def keras_to_frozen_pb(model_in_path, \n",
        "                       model_out_path,\n",
        "                       tensor_out_name=None,\n",
        "                       tensorboard_dir=None):\n",
        "    \"\"\"\n",
        "    Converter that transforms keras model to frozen pb model\n",
        "    \n",
        "    Args:\n",
        "        model_in_path (str): Input model path (.h5) \n",
        "        model_out_path (str): Output model path (dir)\n",
        "        tensor_out_name (str, optional): Specified name of output tensor. \n",
        "                                         If None, it will get default tensor name from keras model.\n",
        "                                         Defaults to None.\n",
        "        tensorboard_dir (str, optional): Output tensorboard dir path for inspecting output model graph.\n",
        "                                         If None, it doesn't generate. \n",
        "                                         Defaults to None.\n",
        "    \"\"\"\n",
        "\n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "        sess = tf.Session()\n",
        "        K.set_session(sess)\n",
        "        K.set_learning_phase(0)\n",
        "\n",
        "        # load the model to graph and sess\n",
        "        model = tf.keras.models.load_model(model_in_path)\n",
        "\n",
        "        # get the tensor_out_name \n",
        "        if tensor_out_name is None:\n",
        "            print('tensor_out_name was not selected manually. it is going to selected automatically.')\n",
        "            if len(model.outputs) > 1:\n",
        "                raise NameError(\"the model has multiple output tensor. Need to specify output tensor name.\")\n",
        "            else:\n",
        "                print('model outptus : {}'.format(model.outputs))\n",
        "                print('selected model outputs : {}'.format(model.outputs[0].name.split(':')[0]))\n",
        "                tensor_out_name = model.outputs[0].name.split(\":\")[0]\n",
        "\n",
        "        # freeze the graph\n",
        "        # Turn all the variables into inline constants inside the graph and save it.\n",
        "\n",
        "        # select one !\n",
        "        # graph.as_graph_def() : method : Returns a serialized GraphDef representation of this graph.\n",
        "        # sess.graph_def : attribute : A serializable version of the underlying TensorFlow graph.\n",
        "        graphdef = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, [tensor_out_name])\n",
        "        graphdef = tf.graph_util.remove_training_nodes(graphdef)\n",
        "\n",
        "        #params : \n",
        "        # graph_or_graph_def\n",
        "        # logdir : directory where to write the graph\n",
        "        # name : filename for the graph\n",
        "        graph_io.write_graph(graphdef, './', model_out_path, as_test = False)\n",
        "\n",
        "    if not tensorboard_dir is None:\n",
        "        tf.summary.FileWriter(logdir=tensorboard_dir, graph_def=graphdef)\n",
        "    \n",
        "    return tensor_out_name\n",
        "\n",
        "\n",
        "\n",
        "input_keras_model = \"modelpath\"\n",
        "output_pb_model = \"outputpath\"\n",
        "\n",
        "model_out_name = keras_to_frozen_pb(input_keras_model, output_pb_model)\n",
        "print(\"output node name : {}\".format(model_out_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hL3hEQMWEJz"
      },
      "source": [
        "### way 1-2 [tensorRT] : (deprecated) frozen pb to tensorRT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "cdMbopMDVkxy",
        "outputId": "15d80b5e-085f-42c6-88ae-e39acf59b40b"
      },
      "source": [
        "import tensorflow as tf\n",
        "import uff\n",
        "import tensorrt as trt\n",
        "\n",
        "def frozen_pb_to_plan(model_path,\n",
        "                      output_path,\n",
        "                      tensor_in_name,\n",
        "                      tensor_out_name,\n",
        "                      input_size,\n",
        "                      data_type=trt.float32,\n",
        "                      max_batch_size=1,\n",
        "                      max_workspace=1<<30,\n",
        "                      tensorboard_dir=None):\n",
        "\n",
        "\n",
        "    # -----\n",
        "    # GFile : File I/O wrappers.\n",
        "      # read more : https://www.tensorflow.org/api_docs/python/tf/io/gfile/GFile\n",
        "    # read modelfile as binary format\n",
        "    graph_def = tf.GraphDef()\n",
        "    with tf.io.gfile.GFile(model_path, \"rb\") as f:\n",
        "        graph_def.ParseFromString(f.read())\n",
        "    \n",
        "    # -----\n",
        "    # uff : a set of utilites to convert trained models\n",
        "    # Converts a TensorFlow frozen graph to a UFF model.\n",
        "    # The uff package contains a set of utilites to convert trained models from various frameworks to a common format.\n",
        "    # from_tensorflow_frozen_model returns serialized UFF MetaGraph (str)\n",
        "      # read more : https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/uff/uff.html\n",
        "    uff_model = uff.from_tensorflow_frozen_model(model_path, [tensor_out_name])\n",
        "\n",
        "    # create uff parser\n",
        "    parser = trt.UffParser()\n",
        "    parser.register_input(tensor_in_name, input_size)\n",
        "    parser.register_output(tensor_out_name)\n",
        "\n",
        "    # -----\n",
        "    # create trt logger and builder\n",
        "    trt_logger = tft.Logger(trt.Logger.INFO)\n",
        "    trt_builder = trt.Builder(trt_logger)\n",
        "\n",
        "    trt_builder.max_batch_size = max_batch_size\n",
        "    trt_builder.max_workspace_size = max_batch_size\n",
        "    trt_builder.fp16_mode = (data_type == trt.float16)\n",
        "\n",
        "    # parse the uff model to trt builder\n",
        "    network = tft_builder.create_network()\n",
        "    parser.parse_buffer(uff_model, network)\n",
        "    \n",
        "    # build optimized inference engine\n",
        "    engine = tft_builder.build_cuda_engine(network)\n",
        "\n",
        "    # save inference engine\n",
        "    with open(output_path, \"wb\") as f:\n",
        "        f.write(engine.serialize())\n",
        "\n",
        "\n",
        "model_path = 'pb model path'\n",
        "output_path = 'output tensorrt plan path'\n",
        "tensor_in_name = 'input node name'\n",
        "tensor_out_name = 'output node name'\n",
        "input_size = [width, height, channel] # order – Input order on which the framework input was originally.\n",
        "\n",
        "frozen_pb_to_plan(model_path,\n",
        "                  output_path,\n",
        "                  tensor_in_name,\n",
        "                  tensor_out_name,\n",
        "                  input_size,\n",
        "                  data_type=trt.float32,\n",
        "                  max_batch_size=1,\n",
        "                  max_workspace=1<<30,\n",
        "                  tensorboard_dir=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-991c7a468196>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    with tf.io.gfile.GFile(model_path, ):\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mGGOigZXadf"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7ECvSFgXaPz"
      },
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.tensorrt as trt\n",
        "\n",
        "\n",
        "def get_frozen_graph(graph_file):\n",
        "  \"\"\"Read Frozen Graph file from disk.\"\"\"\n",
        "  with tf.gfile.FastGFile(graph_file, \"rb\") as f:\n",
        "    graph_def = tf.GraphDef()\n",
        "    graph_def.ParseFromString(f.read())\n",
        "  return graph_def\n",
        "\n",
        "def main():\n",
        "  frozen_graph_def = get_frozen_graph('log/freeze_graph.pb')\n",
        "\n",
        "  output_nodes = ['softmax_tensor']\n",
        "  output_dir = 'tensorrt_dir'\n",
        "\n",
        "  trt_graph_def = trt.create_inference_graph(\n",
        "    frozen_graph_def,\n",
        "    output_nodes,\n",
        "    max_batch_size=1,\n",
        "    max_workspace_size_bytes=(2 << 10) << 20,\n",
        "    precision_mode='FP32')\n",
        "\n",
        "  tf.reset_default_graph()\n",
        "  g = tf.Graph()\n",
        "\n",
        "  with g.as_default():\n",
        "    tf.import_graph_def(\n",
        "      graph_def=trt_graph_def,\n",
        "      return_elements=output_nodes,\n",
        "      name=''\n",
        "    )\n",
        "\n",
        "  with tf.Session(graph=g) as sess:\n",
        "    builder = tf.saved_model.builder.SavedModelBuilder(output_dir)\n",
        "    builder.add_meta_graph_and_variables(\n",
        "      sess,\n",
        "      [tf.saved_model.tag_constants.SERVING]\n",
        "    )\n",
        "    builder.save()\n",
        "    train_writer = tf.summary.FileWriter(output_dir)\n",
        "    train_writer.add_graph(sess.graph)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}