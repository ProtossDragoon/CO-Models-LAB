{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yolov1-Keras-Implementation.ipynb",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "authorship_tag": "ABX9TyMjtC1ipo9YpyyifsEWNx5M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProtossDragoon/CoMoLab/blob/master/CV/Yolov1Keras/Yolov1_Keras_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS5a4BamjxbH"
      },
      "source": [
        "# Yolov1 Model \n",
        "\n",
        "In this notebook I am going to implement YOLOV1 as described in the paper You Only Look Once. The goal is to replicate the model as described in the paper and in the process, understand the nuances of using Keras on a complex problem.\n",
        "\n",
        "- 노트북 소스 원본 자료 : https://www.maskaravivek.com/post/yolov1/\n",
        "- yolo loss 에 대한 고찰 : https://brunch.co.kr/@kmbmjn95/35#comment\n",
        "- yolo 및 one stage detection 깨부수기 : http://machinethink.net/blog/object-detection/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYfx8Q9-jb-0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1MZWYKyjqXB"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ehwq3xVjlJP"
      },
      "source": [
        "Data Preprocessing\n",
        "I would be using VOC 2007 dataset as its size is manageable so it would be easy to run it using Google Colab.\n",
        "\n",
        "First, I download and extract the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIzIGD-fXPML"
      },
      "source": [
        "*아래 데이터를 한 번 다운받았으면 다시는 다운 받을필요가 없겠지요? 매우 오랜 시간이 걸리는 작업이니 주의해 주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGFXsTW7jk4v"
      },
      "source": [
        "%cd /content/gdrive/\"My Drive\"/\n",
        "!rm -r temp\n",
        "!mkdir temp\n",
        "%cd temp\n",
        "\n",
        "\n",
        "!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n",
        "!wget http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\n",
        "\n",
        "!tar xvf VOCtrainval_06-Nov-2007.tar # 현재 디렉터리에 tar file 의 압축을 푸는 코드\n",
        "!tar xvf VOCtest_06-Nov-2007.tar\n",
        "\n",
        "!rm VOCtrainval_06-Nov-2007.tar\n",
        "!rm VOCtest_06-Nov-2007.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKHsaw-3ksjB"
      },
      "source": [
        "%cd /content/gdrive/\"My Drive\"/temp/VOCdevkit\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f2sNIobkMqo"
      },
      "source": [
        "\n",
        "Next, we process the annotations and write the labels in a text file. A text file is easier to consume as compared to XML."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YF4J9HHkMet"
      },
      "source": [
        "%cd /content/gdrive/\"My Drive\"/temp\n",
        "\n",
        "import argparse\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Build Annotations.')\n",
        "parser.add_argument('dir', default='..', help='Annotations.')\n",
        "\n",
        "sets = [('2007', 'train'), ('2007', 'val'), ('2007', 'test')]\n",
        "\n",
        "# 내가 관심있는 class 들과 그에 해당하는 번호\n",
        "classes_num = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5,\n",
        "               'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11,\n",
        "               'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15, 'sheep': 16,\n",
        "               'sofa': 17, 'train': 18, 'tvmonitor': 19}\n",
        "\n",
        "\n",
        "def convert_annotation(year, image_id, f):\n",
        "    in_file = os.path.join('VOCdevkit/VOC%s/Annotations/%s.xml' % (year, image_id))\n",
        "    tree = ET.parse(in_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    for obj in root.iter('object'): # python 반복자 참고 : https://python.bakyeono.net/chapter-7-4.html \n",
        "                                    # xmltree xmlparser iter() 참고 : https://docs.python.org/2/library/xml.etree.elementtree.html#finding-interesting-elements\n",
        "        difficult = obj.find('difficult').text # difficult 가 뭔지는 잘 모르겠음.\n",
        "        cls = obj.find('name').text\n",
        "        classes = list(classes_num.keys())\n",
        "        if cls not in classes or int(difficult) == 1: # 내가 관심있는 class 가 아니면 버림.\n",
        "            continue\n",
        "        cls_id = classes.index(cls)\n",
        "        xmlbox = obj.find('bndbox')\n",
        "        b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text),\n",
        "             int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))\n",
        "        f.write(' ' + ','.join([str(a) for a in b]) + ',' + str(cls_id)) # join 함수 참고 : https://zetawiki.com/wiki/%ED%8C%8C%EC%9D%B4%EC%8D%AC_join()\n",
        "        # 함수가 파일에 쓰는 형식 : \n",
        "        # \" xmin,ymin,xmax,ymax,1 xmin,ymin,xmax,ymax,3 (...object의 개수만큼)\"\n",
        "\n",
        "\n",
        "for year, image_set in sets:\n",
        "  print(year, image_set)\n",
        "  with open(os.path.join('VOCdevkit/VOC%s/ImageSets/Main/%s.txt' % (year, image_set)), 'r') as f: # python context manager 참고 : https://sjquant.tistory.com/12\n",
        "      image_ids = f.read().strip().split() # 파일 입출력 참고 : https://wikidocs.net/26\n",
        "  with open(os.path.join(\"VOCdevkit\", '%s_%s.txt' % (year, image_set)), 'w') as f:\n",
        "      for image_id in image_ids:\n",
        "          f.write('%s/VOC%s/JPEGImages/%s.jpg' % (\"VOCdevkit\", year, image_id))\n",
        "          print('%s/VOC%s/JPEGImages/%s.jpg' % (\"VOCdevkit\", year, image_id))\n",
        "          convert_annotation(year, image_id, f)\n",
        "          f.write('\\n')\n",
        "          # for 문 반복 한번 당 파일에 작성되는 형식 :\n",
        "          # \"VOCdevkit/VOC2007/JPEGImages/이미지명.jpg xmin,ymin,xmax,ymax,1 xmin,ymin,xmax,ymax,3 (...object의 개수만큼)\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0qdlUcDwy0t"
      },
      "source": [
        "!cat 2007_val.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p0TT2dBDPUw"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "Next, I am defining a custom generator that returns a batch of input and outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T4Mvc6HY7cf"
      },
      "source": [
        "Next, I am adding a function to prepare the input and the output. The input is a (448, 448, 3) image and the output is a (7, 7, 30) tensor. The output is based on S x S x (B * 5 +C).\n",
        "\n",
        "S X S is the number of grids B is the number of bounding boxes per grid C is the number of predictions per grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6pWiFBTko4j"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def read(image_path, label):\n",
        "    # 이미지 경로 하나 마다 read() 가 한 번 호출됨.\n",
        "    # label : [(xmin,ymin,xmax,ymax,label), (xmin,ymin,xmax,ymax,label), ... , 해당이미지의 물체개수만큼]\n",
        "\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image_h, image_w = image.shape[0:2]\n",
        "    image = cv2.resize(image, (448, 448))\n",
        "    image = image / 255.\n",
        "    # image shape : [448, 448, 3], dtype : float32, scale : 0~1\n",
        "\n",
        "    label_matrix = np.zeros([7, 7, 30])\n",
        "    for l in label:\n",
        "        l = l.split(',')\n",
        "        l = np.array(l, dtype=np.int)\n",
        "\n",
        "        # bbox parsing\n",
        "        xmin = l[0]\n",
        "        ymin = l[1]\n",
        "        xmax = l[2]\n",
        "        ymax = l[3]\n",
        "\n",
        "        # class\n",
        "        cls = l[4]\n",
        "\n",
        "        # bbox center, scale : 0~1\n",
        "        x = (xmin + xmax) / 2 / image_w\n",
        "        y = (ymin + ymax) / 2 / image_h\n",
        "\n",
        "        # bbox width and height, scale : 0~1\n",
        "        w = (xmax - xmin) / image_w\n",
        "        h = (ymax - ymin) / image_h\n",
        "\n",
        "        # bbox center 의 위치를 7x7 grid 에 구겨넣기\n",
        "        loc = [7 * x, 7 * y]\n",
        "        loc_i = int(loc[1])\n",
        "        loc_j = int(loc[0])\n",
        "\n",
        "        # 7x7 grid 의 해당 위치 안에서 bbox 중심의 위치, scale : 0~1\n",
        "        y = loc[1] - loc_i\n",
        "        x = loc[0] - loc_j\n",
        "\n",
        "        # [0:20] : class score\n",
        "        # [20:24] : x, y, w, h\n",
        "        # [24] : confidence\n",
        "        if label_matrix[loc_i, loc_j, 24] == 0:\n",
        "            label_matrix[loc_i, loc_j, cls] = 1\n",
        "            label_matrix[loc_i, loc_j, 20:24] = [x, y, w, h]\n",
        "            label_matrix[loc_i, loc_j, 24] = 1  # response\n",
        "\n",
        "    return image, label_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2N3hJRucVSc"
      },
      "source": [
        "Next, I am defining a custom generator that returns a batch of input and outputs. \n",
        "\n",
        "<br>\n",
        "\n",
        "### tf.keras.utils.Sequence\n",
        "\n",
        "Base object for fitting to a sequence of data, such as a dataset. \n",
        "\n",
        "\n",
        "Every Sequence must implement then \\_\\_getitem\\_\\_ and the \\_\\_len\\_\\_ methods. If you want to modify your dataset between epochs you may implement on\\_epoch\\_end. The method \\_\\_getitem\\_\\_ should return a complete batch.\n",
        "\n",
        "Notes:\n",
        "Sequence are a safer way to do multiprocessing. This structure guarantees that the network will only train once on each sample per epoch which is not the case with generators.\n",
        "\n",
        "<br>\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o22bMoK2cH5E"
      },
      "source": [
        "class My_Custom_Generator(tf.keras.utils.Sequence) :\n",
        "  \n",
        "  def __init__(self, images, labels, batch_size) :\n",
        "    self.images = images\n",
        "    self.labels = labels\n",
        "    self.batch_size = batch_size\n",
        "    \n",
        "    \n",
        "  def __len__(self) :\n",
        "    # generator 의 반복 횟수를 return 함.\n",
        "    # np.ceil : 숫자 올림\n",
        "    return (np.ceil(len(self.images) / float(self.batch_size))).astype(np.int)\n",
        "  \n",
        "  \n",
        "  def __getitem__(self, idx) :\n",
        "    # batch_x, batch_y 는 각각 이미지 경로들의 집합, 라벨들의 집합임\n",
        "    batch_x = self.images[idx * self.batch_size : (idx+1) * self.batch_size] # ex. [./img31, ./img32, ... , batch_size 개]\n",
        "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size] # ex. [15(xmin),159(ymin),75(xmax),170(ymax),13(label), ... , batch_size 개]\n",
        "\n",
        "    train_image = []\n",
        "    train_label = []\n",
        "\n",
        "    for i in range(0, len(batch_x)):\n",
        "      img_path = batch_x[i]\n",
        "      label = batch_y[i]\n",
        "      image, label_matrix = read(img_path, label)\n",
        "      train_image.append(image)\n",
        "      train_label.append(label_matrix)\n",
        "    return np.array(train_image), np.array(train_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIcxEXM2ld_l"
      },
      "source": [
        "The code snippet below, prepares arrays with inputs and outputs. And we create instances of the generator for our training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RQakseOlbeW"
      },
      "source": [
        "%cd /content/gdrive/\"My Drive\"/temp\n",
        "\n",
        "train_datasets = []\n",
        "val_datasets = []\n",
        "\n",
        "with open(os.path.join(\"VOCdevkit\", '2007_train.txt'), 'r') as f:\n",
        "    train_datasets = train_datasets + f.readlines() # list + list = list\n",
        "with open(os.path.join(\"VOCdevkit\", '2007_val.txt'), 'r') as f:\n",
        "    val_datasets = val_datasets + f.readlines()\n",
        "\n",
        "# train_datasets : [\"VOCdevkit/VOC2007/JPEGImages/009870.jpg 272,70,466,290,11 26,43,315,276,11\", .... , \"\"]\n",
        "\n",
        "X_train = []\n",
        "Y_train = []\n",
        "\n",
        "X_val = []\n",
        "Y_val = []\n",
        "\n",
        "for item in train_datasets:\n",
        "  item = item.replace(\"\\n\", \"\").split(\" \") # \"hello world\\n\" -> [\"hello\", \"world\"]\n",
        "  X_train.append(item[0])\n",
        "  arr = []\n",
        "  for i in range(1, len(item)): # [1:len(item)] : groundtruth bbox 들 하나하나\n",
        "    arr.append(item[i])\n",
        "  Y_train.append(arr)\n",
        "\n",
        "for item in val_datasets:\n",
        "  item = item.replace(\"\\n\", \"\").split(\" \") \n",
        "  X_val.append(item[0])\n",
        "  arr = []\n",
        "  for i in range(1, len(item)):\n",
        "    arr.append(item[i])\n",
        "  Y_val.append(arr)\n",
        "\n",
        "\n",
        "batch_size = 4\n",
        "my_training_batch_generator = My_Custom_Generator(X_train, Y_train, batch_size)\n",
        "my_validation_batch_generator = My_Custom_Generator(X_val, Y_val, batch_size)\n",
        "\n",
        "x_train, y_train = my_training_batch_generator.__getitem__(0)\n",
        "x_val, y_val = my_training_batch_generator.__getitem__(0)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqP9gmvTzedV"
      },
      "source": [
        "## Define a custom output layer\n",
        "\n",
        "We need to reshape the output from the model so we define a custom Keras layer for it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57F67Qzyz7Bh"
      },
      "source": [
        "### tf.keras.layers.Layer\n",
        "\n",
        "This is the class from which all layers inherit.\n",
        "\n",
        "Inherits From: Module\n",
        "\n",
        "call() 함수는, 내부적으로 \\_\\_call()\\_\\_ 함수에서 불리도록 설계되어 있다고 api 문서에 나와 있어요.\n",
        "\n",
        "- 참고 : https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer\n",
        "- 참고 : https://jinmay.github.io/2019/12/03/python/python-callable/\n",
        "- 참고 : https://www.programiz.com/python-programming/methods/dictionary/update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ5H-uWEzHZz"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "class Yolo_Reshape(tf.keras.layers.Layer):\n",
        "  def __init__(self, target_shape):\n",
        "    super(Yolo_Reshape, self).__init__()\n",
        "    self.target_shape = tuple(target_shape)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "    # config return datatype : python dictionary\n",
        "    config.update({\n",
        "        'target_shape': self.target_shape\n",
        "    })\n",
        "    # dict.update()\n",
        "    return config\n",
        "\n",
        "  def call(self, input):\n",
        "    # grids 7x7\n",
        "    S = [self.target_shape[0], self.target_shape[1]]\n",
        "    # classes\n",
        "    C = 20\n",
        "    # no of bounding boxes per grid\n",
        "    B = 2\n",
        "\n",
        "    idx1 = S[0] * S[1] * C\n",
        "    idx2 = idx1 + S[0] * S[1] * B\n",
        "    \n",
        "    # class probabilities\n",
        "    class_probs = K.reshape(input[:, :idx1], (K.shape(input)[0],) + tuple([S[0], S[1], C])) # (batch, S*S*(C+B*2)) -> (batch, S, S, C)\n",
        "    class_probs = K.softmax(class_probs)\n",
        "\n",
        "    #confidence\n",
        "    confs = K.reshape(input[:, idx1:idx2], (K.shape(input)[0],) + tuple([S[0], S[1], B])) # (batch, S*S*(C+B*2)) -> (batch, S, S, B)\n",
        "    confs = K.sigmoid(confs)\n",
        "\n",
        "    # boxes\n",
        "    boxes = K.reshape(input[:, idx2:], (K.shape(input)[0],) + tuple([S[0], S[1], B * 4])) # (batch, S*S*(C+B*2)) -> (batch, S, S, B*4)\n",
        "    boxes = K.sigmoid(boxes)\n",
        "\n",
        "    outputs = K.concatenate([class_probs, confs, boxes]) # finally, (batch, S*S*(C+B*2)) -> (batch, S, S, (C+B*2))\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHYdMJtU8dwj"
      },
      "source": [
        "## Defining the YOLO model\n",
        "\n",
        "Next, we define the model as described in the original paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxZNrXz38bXR"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, InputLayer, Dropout, Flatten, Reshape, LeakyReLU\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "grid_w=7\n",
        "grid_h=7\n",
        "cell_w=64\n",
        "cell_h=64\n",
        "img_w=grid_w*cell_w\n",
        "img_h=grid_h*cell_h\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(Conv2D(filters=64, kernel_size= (7, 7), strides=(1, 1), input_shape =(img_h, img_w, 3), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n",
        "\n",
        "model.add(Conv2D(filters=192, kernel_size= (3, 3), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size= (1, 1), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(Conv2D(filters=256, kernel_size= (3, 3), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n",
        "\n",
        "model.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(Conv2D(filters=1024, kernel_size= (3, 3), strides=(2, 2), padding = 'same'))\n",
        "\n",
        "model.add(Conv2D(filters=1024, kernel_size= (3, 3), activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "model.add(Conv2D(filters=1024, kernel_size= (3, 3), activation=LeakyReLU(alpha=0.1), kernel_regularizer=l2(5e-4)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Dense(1024))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1470, activation='sigmoid'))\n",
        "model.add(Yolo_Reshape(target_shape=(7,7,30)))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ver1q0ho-SG_"
      },
      "source": [
        "## Define a custom learning rate scheduler\n",
        "\n",
        "The paper uses different learning rates for different epochs. So we define a custom Callback function for the learning rate.\n",
        "\n",
        "<br>\n",
        "\n",
        "hasattr()\n",
        "\n",
        "- object의 속성(attribute) 존재를 확인한다.\n",
        "- 만약 argument로 넘겨준 object 에 name 의 속성이 존재하면 True, 아니면 False를 반환한다. \n",
        "\n",
        "<br>\n",
        "\n",
        "tf.keras.callbacks.Callback 의 멤버변수 (Attribute) 들\n",
        "\n",
        "- params (Attribute) : Dict. Training parameters (eg. verbosity, batch size, number of epochs...).\n",
        "- model (Attribute) : model\tInstance of keras.models.Model. **Reference** of the model being trained.\n",
        "\n",
        "<br>\n",
        "\n",
        "*참고 : 이 소스코드처럼 직접 짜내려가도 좋고, https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler 을 활용해보아도 좋을 듯."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGagJNmi-DfW"
      },
      "source": [
        "class CustomLearningRateScheduler(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n",
        "\n",
        "    Arguments:\n",
        "        schedule: a function that takes an epoch index\n",
        "            (integer, indexed from 0) and current learning rate\n",
        "            as inputs and returns a new learning rate as output (float).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, schedule):\n",
        "        super(CustomLearningRateScheduler, self).__init__()\n",
        "        self.schedule = schedule\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None): \n",
        "        if not hasattr(self.model.optimizer, \"lr\"): # https://technote.kr/251\n",
        "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
        "        # Get the current learning rate from model's optimizer.\n",
        "        lr = float(K.get_value(self.model.optimizer.learning_rate))\n",
        "        # Call schedule function to get the scheduled learning rate.\n",
        "        scheduled_lr = self.schedule(epoch, lr)\n",
        "        # Set the value back to the optimizer before this epoch starts\n",
        "        K.set_value(self.model.optimizer.lr, scheduled_lr)\n",
        "        print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, scheduled_lr))\n",
        "\n",
        "\n",
        "LR_SCHEDULE = [\n",
        "    # (epoch to start, learning rate) tuples\n",
        "    (0, 0.01),\n",
        "    (75, 0.001),\n",
        "    (105, 0.0001),\n",
        "]\n",
        "\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n",
        "    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n",
        "        return lr # 현재 learning rate 를 유지하겠다는 생각.\n",
        "    for i in range(len(LR_SCHEDULE)):\n",
        "        if epoch == LR_SCHEDULE[i][0]:\n",
        "          # learning rate 를 변경시켜 주겠다는 생각.\n",
        "            return LR_SCHEDULE[i][1]\n",
        "    return lr\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 잠시 후에 아래와 같은 코드가 탑재될 것임.\n",
        "'''\n",
        "callbacks=[\n",
        "              CustomLearningRateScheduler(lr_schedule),\n",
        "              mcp_save\n",
        "          ])\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWftdpA6DEiw"
      },
      "source": [
        "## Define the loss function\n",
        "\n",
        "Next, we would be defining a custom loss function to be used in the model. Take a look at this blog post to understand more about the loss function used in YOLO.\n",
        "\n",
        "I understood the loss function but didn’t implement it on my own. I took the implementation as it is from this Github repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KoSfw9cC-Lx"
      },
      "source": [
        "def xywh2minmax(xy, wh):\n",
        "    xy_min = xy - wh / 2\n",
        "    xy_max = xy + wh / 2\n",
        "\n",
        "    return xy_min, xy_max\n",
        "\n",
        "\n",
        "def iou(pred_mins, pred_maxes, true_mins, true_maxes):\n",
        "    intersect_mins = K.maximum(pred_mins, true_mins)\n",
        "    intersect_maxes = K.minimum(pred_maxes, true_maxes)\n",
        "    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.)\n",
        "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
        "\n",
        "    pred_wh = pred_maxes - pred_mins\n",
        "    true_wh = true_maxes - true_mins\n",
        "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
        "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
        "\n",
        "    union_areas = pred_areas + true_areas - intersect_areas\n",
        "    iou_scores = intersect_areas / union_areas\n",
        "\n",
        "    return iou_scores\n",
        "\n",
        "\n",
        "def yolo_head(feats):\n",
        "    # Dynamic implementation of conv dims for fully convolutional model.\n",
        "    conv_dims = K.shape(feats)[1:3]  # assuming channels last\n",
        "    # In YOLO the height index is the inner most iteration.\n",
        "    conv_height_index = K.arange(0, stop=conv_dims[0])\n",
        "    conv_width_index = K.arange(0, stop=conv_dims[1])\n",
        "    conv_height_index = K.tile(conv_height_index, [conv_dims[1]])\n",
        "\n",
        "    # TODO: Repeat_elements and tf.split doesn't support dynamic splits.\n",
        "    # conv_width_index = K.repeat_elements(conv_width_index, conv_dims[1], axis=0)\n",
        "    conv_width_index = K.tile(\n",
        "        K.expand_dims(conv_width_index, 0), [conv_dims[0], 1])\n",
        "    conv_width_index = K.flatten(K.transpose(conv_width_index))\n",
        "    conv_index = K.transpose(K.stack([conv_height_index, conv_width_index]))\n",
        "    conv_index = K.reshape(conv_index, [1, conv_dims[0], conv_dims[1], 1, 2])\n",
        "    conv_index = K.cast(conv_index, K.dtype(feats))\n",
        "\n",
        "    conv_dims = K.cast(K.reshape(conv_dims, [1, 1, 1, 1, 2]), K.dtype(feats))\n",
        "\n",
        "    box_xy = (feats[..., :2] + conv_index) / conv_dims * 448\n",
        "    box_wh = feats[..., 2:4] * 448\n",
        "\n",
        "    return box_xy, box_wh\n",
        "\n",
        "\n",
        "def yolo_loss(y_true, y_pred):\n",
        "    label_class = y_true[..., :20]   # ? * 7 * 7 * 20\n",
        "    label_box = y_true[..., 20:24]   # ? * 7 * 7 * 4\n",
        "    response_mask = y_true[..., 24]  # ? * 7 * 7\n",
        "    response_mask = K.expand_dims(response_mask)  # ? * 7 * 7 * 1\n",
        "\n",
        "    predict_class = y_pred[..., :20]    # ? * 7 * 7 * 20\n",
        "    predict_trust = y_pred[..., 20:22]  # ? * 7 * 7 * 2\n",
        "    predict_box = y_pred[..., 22:]      # ? * 7 * 7 * 8\n",
        "\n",
        "    _label_box = K.reshape(label_box, [-1, 7, 7, 1, 4])\n",
        "    _predict_box = K.reshape(predict_box, [-1, 7, 7, 2, 4])\n",
        "\n",
        "    label_xy, label_wh = yolo_head(_label_box)  # ? * 7 * 7 * 1 * 2, ? * 7 * 7 * 1 * 2\n",
        "    label_xy = K.expand_dims(label_xy, 3)       # ? * 7 * 7 * 1 * 1 * 2\n",
        "    label_wh = K.expand_dims(label_wh, 3)       # ? * 7 * 7 * 1 * 1 * 2\n",
        "    label_xy_min, label_xy_max = xywh2minmax(label_xy, label_wh)  # ? * 7 * 7 * 1 * 1 * 2, ? * 7 * 7 * 1 * 1 * 2\n",
        "\n",
        "    predict_xy, predict_wh = yolo_head(_predict_box)  # ? * 7 * 7 * 2 * 2, ? * 7 * 7 * 2 * 2\n",
        "    predict_xy = K.expand_dims(predict_xy, 4)         # ? * 7 * 7 * 2 * 1 * 2\n",
        "    predict_wh = K.expand_dims(predict_wh, 4)         # ? * 7 * 7 * 2 * 1 * 2\n",
        "    predict_xy_min, predict_xy_max = xywh2minmax(predict_xy, predict_wh)  # ? * 7 * 7 * 2 * 1 * 2, ? * 7 * 7 * 2 * 1 * 2\n",
        "\n",
        "    iou_scores = iou(predict_xy_min, predict_xy_max, label_xy_min, label_xy_max)  # ? * 7 * 7 * 2 * 1\n",
        "    best_ious = K.max(iou_scores, axis=4)               # ? * 7 * 7 * 2\n",
        "    best_box = K.max(best_ious, axis=3, keepdims=True)  # ? * 7 * 7 * 1\n",
        "\n",
        "    box_mask = K.cast(best_ious >= best_box, K.dtype(best_ious))  # ? * 7 * 7 * 2\n",
        "\n",
        "    no_object_loss = 0.5 * (1 - box_mask * response_mask) * K.square(0 - predict_trust)\n",
        "    object_loss = box_mask * response_mask * K.square(1 - predict_trust)\n",
        "    confidence_loss = no_object_loss + object_loss\n",
        "    confidence_loss = K.sum(confidence_loss)\n",
        "\n",
        "    class_loss = response_mask * K.square(label_class - predict_class)\n",
        "    class_loss = K.sum(class_loss)\n",
        "\n",
        "    _label_box = K.reshape(label_box, [-1, 7, 7, 1, 4])\n",
        "    _predict_box = K.reshape(predict_box, [-1, 7, 7, 2, 4])\n",
        "\n",
        "    label_xy, label_wh = yolo_head(_label_box)        # ? * 7 * 7 * 1 * 2, ? * 7 * 7 * 1 * 2\n",
        "    predict_xy, predict_wh = yolo_head(_predict_box)  # ? * 7 * 7 * 2 * 2, ? * 7 * 7 * 2 * 2\n",
        "\n",
        "    box_mask = K.expand_dims(box_mask)\n",
        "    response_mask = K.expand_dims(response_mask)\n",
        "\n",
        "    box_loss = 5 * box_mask * response_mask * K.square((label_xy - predict_xy) / 448)\n",
        "    box_loss += 5 * box_mask * response_mask * K.square((K.sqrt(label_wh) - K.sqrt(predict_wh)) / 448)\n",
        "    box_loss = K.sum(box_loss)\n",
        "\n",
        "    loss = confidence_loss + class_loss + box_loss\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}